{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45928c38",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# BLIP: IMAGE CAPTIONING (BATCH PROCESSING)\n",
    "# ===============================\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "import kagglehub\n",
    "kaggle_root = kagglehub.dataset_download('nodoubttome/skin-cancer9-classesisic')\n",
    "DATA_ROOT = os.path.join(\n",
    "    kaggle_root,\n",
    "    \"Skin cancer ISIC The International Skin Imaging Collaboration\",\n",
    "    \"Train\"\n",
    ")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 8  # Adjust based on GPU memory\n",
    "\n",
    "# ---------- LOAD IMAGE PATHS + LABELS ----------\n",
    "image_paths, labels = [], []\n",
    "root_path = Path(DATA_ROOT)\n",
    "\n",
    "for label_dir in sorted(root_path.iterdir()):\n",
    "    if label_dir.is_dir():\n",
    "        for img_file in label_dir.iterdir():\n",
    "            if img_file.suffix.lower() in ['.png', '.jpg', '.jpeg']:\n",
    "                image_paths.append(str(img_file))\n",
    "                labels.append(label_dir.name)\n",
    "\n",
    "df = pd.DataFrame({\"image_path\": image_paths, \"label\": labels})\n",
    "print(f\"✅ Loaded {len(df)} images across {df['label'].nunique()} classes.\")\n",
    "\n",
    "# ---------- LOAD BLIP MODEL ----------\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(DEVICE)\n",
    "\n",
    "# ---------- GENERATE CAPTIONS IN BATCHES ----------\n",
    "captions = []\n",
    "for i in tqdm(range(0, len(df), BATCH_SIZE), desc=\"Generating captions in batches\"):\n",
    "    batch_paths = df[\"image_path\"].iloc[i:i+BATCH_SIZE].tolist()\n",
    "    images = [Image.open(p).convert(\"RGB\") for p in batch_paths]\n",
    "    inputs = processor(images=images, return_tensors=\"pt\").to(DEVICE)\n",
    "    out = model.generate(**inputs)\n",
    "    batch_captions = processor.batch_decode(out, skip_special_tokens=True)\n",
    "    captions.extend(batch_captions)\n",
    "\n",
    "df[\"caption\"] = captions\n",
    "\n",
    "# ---------- SHOW SAMPLE CAPTIONS ----------\n",
    "print(\"\\nSample Captions:\")\n",
    "print(df[[\"image_path\", \"label\", \"caption\"]].head())\n",
    "\n",
    "# ---------- SAVE CAPTIONS ----------\n",
    "df.to_csv(\"captions_dataset.csv\", index=False)\n",
    "print(\"✅ Captions saved to captions_dataset.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
